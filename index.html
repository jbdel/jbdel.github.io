<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="fr">
<head>
    <title>Jean-Benoit Delbrouck</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <meta name="author" content="Jean-Benoit Delbrouck"/>
    <meta name="keywords"
          content="Jean-Benoit, Delbrouck, NLP, Neural machine translation, multimodal interfaces, interaction, cognition, robotics"/>
    <meta name="description" content="Jean-Benoit Delbrouck"/>
    <meta name="robots" content="all"/>

    <link rel="shortcut icon" href="https://www-media.stanford.edu/assets/favicon/favicon-128.png" type="image/x-icon"/>


    <style>
        body {
            font-family: Verdana, Arial, Helvetica, sans-serif;
            font-size: 12px;
            color: black;
        }

        #papers li {
            margin-bottom: 10px;
        }

        a:link {
            color: blue;
            text-decoration: underline;
        }

        a:visited {
            color: blue;
            text-decoration: none;
        }



    </style>

    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js" type="text/javascript"></script>
    <script>
        $(function () {
            var $element = $('.highlight1');
            setInterval(function () {
                $element.fadeIn(1000, function () {
                    $element.fadeOut(1500, function () {
                        $element.fadeIn(1500)
                    });
                });
            }, 750);
        });

    </script>

</head>
<body>
<p>
    <img src="img/fpms_logo.png" height="50px"/> &nbsp; &nbsp; &nbsp;
    <img src="img/stanford_logo.png" height="50px"/></p>


<fieldset style="width:30%">
    <legend>Infos</legend>


    <img src="https://aimi.stanford.edu/sites/g/files/sbiybj20451/files/styles/medium_square/public/media/image/image5_0.png?h=f4e62a0a&itok=euaj9VoF"
         width="150px" style="float:left;margin-right:5px;">
    Jean-Benoit Delbrouck<br/>
    Email : jbdel at stanford dot edu<br/>
    Email : Jean-Benoit.delbrouck at umons dot ac dot be<br/>
    Adress : Address: 1701 Page Mill Rd, Palo Alto, CA 94304, United States<br/>
</fieldset>

<fieldset style="width:30%">
    <legend>Blurb</legend>
    My research lies at the intersection of vision and language in medical AI. I'm interested in using paired medical
    images and
    radiology reports to improve the performances of trained models on a wide range of benchmarks. More specifically, I
    focus on NLP techniques to extract rich semantic features from the reports to combine them with visual
    representations. I have had the chance to publish my top-tier AI conferences such as ACL, EMNLP and ICLR.
</fieldset>

<fieldset style="width:30%">
    <legend><span class="highlight1" style="color:red">News</span></legend>
    - We are organizing the BioNLP workshop at ACL 2023, check it out!<br/>
    https://vilmedic.app/misc/bionlp23/sharedtask/<br/>
    https://vilmedic.app/misc/bionlp23/leaderboard/<br/>
    -We generate chest x-rays using stable diffusion, <a href="https://arxiv.org/abs/2211.12737">check it out</a><br/>
</fieldset>


<fieldset style="width:30%">
    <legend><span>2022 recap</span></legend>
    Early 2022, we got our medical AI library ViLMedic accepted to <a href="https://aclanthology.org/2022.acl-demo.3/">ACL</a>.
    ViLMedic offers dozens of pretrained models at the intersection of vision and language in the medical field.
    ViLMedic was leveraged to evaluate Domino,
    a systematic error discovery method, accepted as <a href="https://openreview.net/forum?id=FPCMqjI0jXN">Spotlight at
    ICLR 2022</a>. A new paper in Radiology Report Generation was accepted at <a
        href="https://arxiv.org/abs/2210.12186">EMNLP 2022</a> where we leverage graph-based
    semantic radiologists annotations to evaluate the factual correctness of AI-generated radiology reports. We released
    a new Radiology report summarization dataset of 70k samples with dozens of new modality-anatomy pairs, accepted at
    <a href="">ML4H</a>.
    Last but not least, we fine-tuned stable diffusion on chest x-rays. <a
        href="https://stanfordmimi.github.io/RoentGen/">Check it out, its amazing</a> :)
</fieldset>


<fieldset style="width:30%">
    <legend>Publications</legend>


    2023
    <ul id="papers">
        <li>
            Toward expanding the scope of radiology report summarization to multiple anatomies and modalities
            <b>ACL 2023</b>
        </li>
        <li>
            RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models
            <b>ACL 2023 bioNLP</b>
        </li>
        <li>
            Overview of the RadSum23 Shared Task on Multi-modal and Multi-anatomical Radiology Report Summarization
            <b>ACL 2023 bioNLP</b>
        </li>

    </ul>
    2022
    <ul id="papers">
        <li>
            Domino: Discovering systematic errors with cross-modal embeddings
            <b>ICLR 2022 <i style="color:red;">(Oral Spotlight)</i></b>
        </li>
        <li>
            Improving the Factual Correctness of Radiology Report Generation
            with Semantic Rewards
            <b>EMNLP 2022</b>
        </li>
        <li>
            ViLMedic: a modular framework for research at the intersection of
            vision and language in medical AI
            <b>ACL 2022</b>
        </li>

    </ul>
    2021
    <ul id="papers">

        <li>
            Multimodal Radiology Report Summarization
            <b>NAACL 2021</b>
        </li>
        <li>
            Out of Distribution Detection for Medical Images
            <b>Proceedings of MICCAI, 2021 <i style="color:red;">(Oral Spotlight)</i></b>
        </li>

    </ul>


    2020

    <ul id="papers">
        <li>J.B. Delbrouck, N. Tits, S. Dupont. Modulated Fusion using Transformer for Linguistic-Acoustic Emotion
            Recognition
            <a href="https://www.aclweb.org/anthology/2020.nlpbt-1.1/">[Link]</a>
            <a href="https://www.aclweb.org/anthology/2020.nlpbt-1.1.bib">[bib]</a>
            <a href="https://github.com/jbdel/modulated_fusion_transformer">[Code]</a>
            - <b>First International Workshop on Natural Language Processing Beyond Text, EMNLP 2020, Online</b>

        </li>

        <li>J.B. Delbrouck, N. Tits, M. Brousmiche, S. Dupont. A Transformer-based joint-encoding for Emotion
            Recognition and Sentiment Analysis
            <i style="color:red;"> Challenge 1st place and best Paper Award</i>
            <a href="https://www.aclweb.org/anthology/2020.challengehml-1.1/">[Link]</a>
            <a href="https://www.aclweb.org/anthology/2020.challengehml-1.1.bib">[bib]</a>
            <a href="https://github.com/jbdel/MOSEI_UMONS">[Code]</a>
            - <b>Second Grand-Challenge and Workshop on Multimodal Language, ACL 2020, Seattle, USA</b>

        </li>
    </ul>

    2019

    <ul id="papers">
        <li>J.B. Delbrouck, S. Dupont. Adversarial reconstruction for Multi-modal Machine Translation
            <i style="color:red;">
                <a href="https://arxiv.org/abs/1910.02766">[Link]</a>
                <a href="">[bib]</a>
            </i>
        </li>

        <li>J.B. Delbrouck, Vanderplaetse B., S. Dupont. Modulated Self-attention Convolutional Network for VQA
            <i style="color:red;">
                <a href="">[Link]</a>
                <a href="">[bib]</a>
            </i>
            - <b>ViGiL Workshop, NeurIPS 2020, Vancouver, Canada</b>
        </li>

        <li>J.B. Delbrouck, S. Dupont. Can adversarial training learn image captioning ?
            <i style="color:red;">
                <a href="">[Link]</a>
                <a href="">[bib]</a>
                <a href="https://github.com/bastienvanderplaetse/gan-image-captioning">[Code]</a>
            </i>
            - <b>ViGiL Workshop, NeurIPS 2020, Vancouver, Canada</b>
        </li>


    </ul>

    2018
    <ul id="papers">
        <li>J.B. Delbrouck, S. Dupont. Umons submission for wmt18 multimodal translation task
            <i style="color:red;"> Challenge 1st place</i>
            <a href="http://statmt.org/wmt18/pdf/WMT071.pdf">[Link]</a>
            <a href="http://statmt.org/wmt18/bib/WMT071.bib">[bib]</a>
            <a href="https://github.com/jbdel/WMT18_MNMT">[Code]</a>
            - <b>Proceedings of the Third Conference on Machine Translation, Brussels, Belgium</b>
        </li>

        <li>J.B. Delbrouck, S. Dupont. Object-oriented Targets for Visual Navigation using Rich Semantic Representations
            <i style="color:red;">
                <a href="https://nips2018vigil.github.io/static/papers/accepted/15.pdf">[Link]</a>
                <a href="">[bib]</a>
            </i>
            - <b>ViGiL Workshop, NIPS 2019, Montreal, Canada</b>
        </li>


    </ul>

    2017
    <ul id="papers">


        <li>J.B. Delbrouck, S. Dupont. Modulating and attending the source image during encoding improves Multimodal
            Translation
            <i style="color:red;">
                <a href="https://arxiv.org/pdf/1712.03449">[Link]</a>
                <a href="">[bib]</a>
            </i>
            - <b>NIPS, Long Beach, CA, USA</b>
        </li>


        <li>J.B. Delbrouck, S. Dupont, O. Seddati. Visually Grounded Word Embeddings and Richer Visual Features for
            Improving Multimodal Neural Machine Translation
            <i style="color:red;">
                <a href="http://www.isca-speech.org/archive/GLU_2017/pdfs/GLU2017_paper_15.pdf">[Link]</a>
                <a href="http://www.isca-speech.org/archive/GLU_2017/abstracts/GLU2017_paper_15.html">[bib]</a>
            </i>
            - <b>Interspeech, Stockholm, Sweden</b>
        </li>

        <li>J.B. Delbrouck, S. Dupont. An empirical study on the effectiveness of images in Multimodal Neural Machine
            Translation
            <i style="color:red;">
                <a href="http://www.aclweb.org/anthology/D/D17/D17-1095.pdf">[Link]</a>
                <a href="http://www.aclweb.org/anthology/D/D17/D17-1095.bib">[bib]</a>
            </i>
            - <b>EMNLP, Copenhagen, Denmark</b>
        </li>

        <li>J.B. Delbrouck, S. Dupont. Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation
            <i style="color:red;"><a href="https://arxiv.org/pdf/1703.08084.pdf">[Link]</a>
                <a href="http://dblp.uni-trier.de/rec/bibtex/journals/corr/DelbrouckD17">[bib]</a>
                <a href="https://github.com/jbdel/mmt_cbn">[Code]</a>
            </i>
        </li>
    </ul>


</fieldset>


<fieldset style="width:30%">
    <legend>Teaching</legend>

    <b>Lecturer</b>
    <ul>
        <li>BIOMEDIN 260 - Stanford (2021)</li>
        <li>Artificial Intelligence - Haute école Léonard de Vinci (BA3)(2017-2019) <a href="teaching/index.html">[Syllabus]</a>
        </li>
        <li>Signal Processing - Polytechnic Mons (MA1) (2016-2020) <a href="teaching/SP/TraitSig.pdf">[Syllabus]</a> <a
                href="teaching/index.html">[My notes]</a></li>
        <li>Databases - Haute école Léonard de Vinci (BA2)(2016-2017)</li>
        <li>Data Structures - Haute école Léonard de Vinci (BA1)(2016-2017)</li>
    </ul>


    <b>Mentoring</b><br/>

    Groups:
    <ul>
        <p>
            AIMI Summer Research Intership, Mentor of 25+ students , 2022<br/>
            Stanford University, CA, United-States<br/>
            https://aimi.stanford.edu/engage/summer-research-internship</p>

        <p>HAI Summer Research Intership, Mentor of 25+ students, 2022<br/>
            Stanford University, CA, United-States<br/>
            https://hai.stanford.edu/stanford-ai4all</p>
    </ul>


    Individuals:
    <ul>
        <b>Stanford University, Palo Alto, CA, California</b>
        <li>Nina Du, Incremental and federated learning for medical data (Graduate), 2021</li>
        <li>Cassie Zhang, Radiology Report Summarization (Graduate), 2021</li>
        <li>Oliver Zhang, Out-of-distribution detection for medical images (Graduate), 2021</li>
        <li>Kent Vainio & Michael Cooper, Out-of-distribution detection for medical images (Graduate), 2020</li>
        <b>Mons Polytechnique, Mons, Belgium</b>
        <li>Mathieu Plapied, Learning languages using chatbots (BA3), 2021</li>
        <li>Kilian Valentin, Approximate the mapping from visual information to condensed textual descriptions (BA3),
            2021
        </li>
        <li>Marie Macaspac, Visual Question Answering in the medical domain (BA3), 2021</li>
        <li>Nader Karekezi & Romain Detrait, Tackling the hidden stratification problem in medical imaging (BA3), 2021
        </li>
        <li>Sakir Ozturk, Deep Learning at the intersection of vision and language (BA3), 2020</li>
        <li>Robin Jacob, Dense annotations for image captioning (MA2), 2019</li>
        <li>Arnaud Vella, Adversarial training for visual question answering (MA2), 2019</li>
        <li>Bastien Vanderplaetse, Can adversarial networks learn image captioning without pretraining (MA2), 2019</li>
        <li>Michael Rombaux, Learn to collaborate with adversarial training (MA2), 2019</li>
        <li>Jason Bury, Reinforcement Learning in Games (MA2), 2017</li>
        <li>Arnaud Vella, Automatic image captioning using AI (MA1), 2017</li>
    </ul>

    <b>Jurys</b>

    <ul>
        <li>2018 - 2020: Polytechnic Mons - Electrical engineering</li>
        <li>President - June 2018 - Haute école Léonard de Vinci</li>
        <li>President - June 2018 - Haute école Léonard de Vinci</li>
        <li>June 2017 - Haute école Léonard de Vinci</li>
    </ul>

</fieldset>


</body>


</html>
